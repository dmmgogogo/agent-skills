# PyTorch Skill

## ğŸ“š å·¥å…·ç®€ä»‹

**PyTorch** æ˜¯Facebookå¼€å‘çš„å¼€æºæ·±åº¦å­¦ä¹ æ¡†æ¶,å› å…¶æ˜“ç”¨æ€§å’ŒåŠ¨æ€è®¡ç®—å›¾è€Œæˆä¸ºAIç ”ç©¶å’Œç”Ÿäº§çš„é¦–é€‰æ¡†æ¶ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **åŠ¨æ€è®¡ç®—å›¾**: çµæ´»çš„æ¨¡å‹æ„å»º
- **è‡ªåŠ¨å¾®åˆ†**: ç®€åŒ–æ¢¯åº¦è®¡ç®—
- **GPUåŠ é€Ÿ**: æ— ç¼CUDAæ”¯æŒ
- **ä¸°å¯Œç”Ÿæ€**: torchvision, torchaudio, torchtextç­‰
- **ç”Ÿäº§éƒ¨ç½²**: TorchScript, ONNXæ”¯æŒ
- **åˆ†å¸ƒå¼è®­ç»ƒ**: å†…ç½®åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

### GitHubä¿¡æ¯
- **Stars**: ä¸šç•Œé¢†å…ˆ
- **ç¤¾åŒº**: æœ€æ´»è·ƒçš„æ·±åº¦å­¦ä¹ ç¤¾åŒº
- **ä»“åº“**: https://github.com/pytorch/pytorch
- **å®˜æ–¹æ–‡æ¡£**: https://pytorch.org/docs/

### é€‚ç”¨åœºæ™¯
âœ… æ·±åº¦å­¦ä¹ ç ”ç©¶
âœ… è®¡ç®—æœºè§†è§‰
âœ… è‡ªç„¶è¯­è¨€å¤„ç†(NLP)
âœ… å¼ºåŒ–å­¦ä¹ 
âœ… GPTã€Llamaç­‰å¤§è¯­è¨€æ¨¡å‹å¼€å‘
âœ… ç”Ÿæˆå¼AI(GANs, Diffusion Models)

---

## ğŸ”§ å®‰è£…å’Œé…ç½®

### CPUç‰ˆæœ¬

```bash
# ä½¿ç”¨pipå®‰è£…
pip install torch torchvision torchaudio --break-system-packages
```

### GPUç‰ˆæœ¬ (CUDA)

```bash
# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --break-system-packages

# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --break-system-packages
```

### éªŒè¯å®‰è£…

```python
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU device: {torch.cuda.get_device_name(0)}")
```

---

## ğŸ’» ä»£ç ç¤ºä¾‹

### 1. å¼ é‡åŸºç¡€æ“ä½œ

```python
import torch

# åˆ›å»ºå¼ é‡
x = torch.tensor([[1, 2], [3, 4]])
y = torch.zeros(2, 3)
z = torch.randn(2, 2)  # éšæœºæ­£æ€åˆ†å¸ƒ

# å¼ é‡è¿ç®—
a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])

print(a + b)  # åŠ æ³•
print(a * b)  # é€å…ƒç´ ä¹˜æ³•
print(torch.dot(a, b))  # ç‚¹ç§¯

# å½¢çŠ¶æ“ä½œ
x = torch.randn(2, 3, 4)
print(x.shape)
x_reshaped = x.view(2, 12)  # é‡å¡‘
x_transposed = x.permute(2, 0, 1)  # è½¬ç½®

# GPUæ“ä½œ
if torch.cuda.is_available():
    x_gpu = x.cuda()  # ç§»åˆ°GPU
    x_cpu = x_gpu.cpu()  # ç§»å›CPU
```

### 2. è‡ªåŠ¨å¾®åˆ†

```python
# å¯ç”¨æ¢¯åº¦è¿½è¸ª
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2 + 3 * x + 1

# åå‘ä¼ æ’­
y.backward()
print(f"dy/dx = {x.grad}")  # 4 * 2 + 3 = 11

# å¤šå˜é‡
x = torch.randn(3, requires_grad=True)
y = x * 2
z = y.mean()

z.backward()
print(x.grad)
```

### 3. æ„å»ºç¥ç»ç½‘ç»œ

```python
import torch.nn as nn
import torch.nn.functional as F

# å®šä¹‰ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# å®ä¾‹åŒ–æ¨¡å‹
model = SimpleNet(input_size=784, hidden_size=128, num_classes=10)
print(model)

# æŸ¥çœ‹å‚æ•°
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")
```

### 4. å·ç§¯ç¥ç»ç½‘ç»œ(CNN)

```python
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 7 * 7)  # Flatten
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

model = ConvNet()
```

### 5. å®Œæ•´è®­ç»ƒå¾ªç¯

```python
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# å‡†å¤‡æ•°æ®
X_train = torch.randn(1000, 784)
y_train = torch.randint(0, 10, (1000,))
dataset = TensorDataset(X_train, y_train)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
model = SimpleNet(784, 128, 10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒå¾ªç¯
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch_x, batch_y in dataloader:
        # å‰å‘ä¼ æ’­
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)

        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(dataloader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")
```

### 6. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# ä¿å­˜æ•´ä¸ªæ¨¡å‹
torch.save(model, 'model.pth')
model = torch.load('model.pth')

# åªä¿å­˜å‚æ•°(æ¨è)
torch.save(model.state_dict(), 'model_weights.pth')
model = SimpleNet(784, 128, 10)
model.load_state_dict(torch.load('model_weights.pth'))

# ä¿å­˜æ£€æŸ¥ç‚¹
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss
}
torch.save(checkpoint, 'checkpoint.pth')

# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

### 7. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

```python
import torchvision.models as models

# åŠ è½½é¢„è®­ç»ƒResNet
resnet = models.resnet50(pretrained=True)

# å†»ç»“å‚æ•°
for param in resnet.parameters():
    param.requires_grad = False

# ä¿®æ”¹æœ€åä¸€å±‚ç”¨äºè¿ç§»å­¦ä¹ 
num_features = resnet.fc.in_features
resnet.fc = nn.Linear(num_features, 10)  # 10ä¸ªç±»åˆ«

# åªè®­ç»ƒæœ€åä¸€å±‚
optimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. è®¾å¤‡ç®¡ç†

```python
# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# å°†æ¨¡å‹å’Œæ•°æ®ç§»åˆ°è®¾å¤‡
model = model.to(device)
inputs = inputs.to(device)
labels = labels.to(device)

# æˆ–è€…ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with torch.cuda.device(0):
    model = model.cuda()
```

### 2. æ··åˆç²¾åº¦è®­ç»ƒ(èŠ‚çœæ˜¾å­˜)

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in dataloader:
    optimizer.zero_grad()

    # è‡ªåŠ¨æ··åˆç²¾åº¦
    with autocast():
        output = model(data)
        loss = criterion(output, target)

    # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### 3. å­¦ä¹ ç‡è°ƒåº¦

```python
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR

# é˜¶æ¢¯å¼è¡°å‡
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

# ä½™å¼¦é€€ç«
scheduler = CosineAnnealingLR(optimizer, T_max=100)

# è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for epoch in range(num_epochs):
    train(...)
    scheduler.step()
```

### 4. æ¢¯åº¦è£å‰ª(é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)

```python
# åœ¨optimizer.step()ä¹‹å‰
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### 5. æ•°æ®å¢å¼º

```python
from torchvision import transforms

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])
```

---

## âš ï¸ å¸¸è§é—®é¢˜å’Œæ³¨æ„äº‹é¡¹

### é—®é¢˜1: CUDA Out of Memory

```python
# è§£å†³æ–¹æ¡ˆ:
# 1. å‡å°batch size
# 2. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
# 3. æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
for i, (data, target) in enumerate(dataloader):
    output = model(data)
    loss = criterion(output, target) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 4. æ¸…ç†æ˜¾å­˜
torch.cuda.empty_cache()
```

### é—®é¢˜2: æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

```python
# ä½¿ç”¨æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# ä½¿ç”¨Batch Normalization
self.bn = nn.BatchNorm2d(num_features)

# ä½¿ç”¨æ®‹å·®è¿æ¥
class ResidualBlock(nn.Module):
    def forward(self, x):
        return x + self.layer(x)
```

### é—®é¢˜3: æ¨¡å‹ä¸æ”¶æ•›

```python
# æ£€æŸ¥æ¸…å•:
# 1. å­¦ä¹ ç‡æ˜¯å¦åˆé€‚
# 2. æ•°æ®æ˜¯å¦æ ‡å‡†åŒ–
# 3. æƒé‡åˆå§‹åŒ–æ˜¯å¦æ­£ç¡®
def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

model.apply(init_weights)

# 4. æ·»åŠ æ¢¯åº¦ç›‘æ§
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name} gradient: {param.grad.norm()}")
```

---

## ğŸ“– è¿›é˜¶èµ„æº

### å®˜æ–¹èµ„æº
- [PyTorchå®˜æ–¹æ•™ç¨‹](https://pytorch.org/tutorials/)
- [PyTorchæ–‡æ¡£](https://pytorch.org/docs/stable/index.html)
- [PyTorchç¤ºä¾‹](https://github.com/pytorch/examples)

### æ¨èè¯¾ç¨‹
- fast.ai - Practical Deep Learning
- Stanford CS231n (ä½¿ç”¨PyTorchç‰ˆæœ¬)
- PyTorchå®˜æ–¹60åˆ†é’Ÿå…¥é—¨

### ç›¸å…³åº“
- **torchvision**: è®¡ç®—æœºè§†è§‰å·¥å…·
- **torchaudio**: éŸ³é¢‘å¤„ç†
- **torchtext**: NLPå·¥å…·
- **pytorch-lightning**: é«˜çº§è®­ç»ƒæ¡†æ¶

---

## ğŸ”— ç›¸å…³Skills

- **huggingface-skill**: Transformeræ¨¡å‹
- **numpy-skill**: æ•°ç»„æ“ä½œåŸºç¡€
- **matplotlib-skill**: è®­ç»ƒå¯è§†åŒ–
- **jupyter-skill**: äº¤äº’å¼å®éªŒ

---

**æœ€åæ›´æ–°**: 2026-01-22
**ç‰ˆæœ¬**: 2.x
