# Hugging Face Transformers Skill

## ğŸ“š å·¥å…·ç®€ä»‹

**Hugging Face Transformers** æ˜¯æœ€æµè¡Œçš„é¢„è®­ç»ƒæ¨¡å‹åº“,æä¾›äº†æ•°åƒä¸ªæœ€å…ˆè¿›çš„NLPã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘å¤„ç†æ¨¡å‹ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **é¢„è®­ç»ƒæ¨¡å‹**: 120,000+ æ¨¡å‹å¯ç”¨
- **å¤šæ¡†æ¶æ”¯æŒ**: PyTorch, TensorFlow, JAX
- **æ˜“ç”¨API**: ç®€å•å‡ è¡Œä»£ç å³å¯ä½¿ç”¨SOTAæ¨¡å‹
- **ä»»åŠ¡Pipeline**: å¼€ç®±å³ç”¨çš„ä»»åŠ¡ç®¡é“
- **æ¨¡å‹Hub**: ç¤¾åŒºå…±äº«çš„æ¨¡å‹ä»“åº“
- **å¾®è°ƒå·¥å…·**: Trainer APIç®€åŒ–è®­ç»ƒæµç¨‹

### GitHubä¿¡æ¯
- **Stars**: 120,000+
- **ä»“åº“**: https://github.com/huggingface/transformers
- **å®˜æ–¹æ–‡æ¡£**: https://huggingface.co/docs/transformers
- **Model Hub**: https://huggingface.co/models

### é€‚ç”¨åœºæ™¯
âœ… æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æ
âœ… å‘½åå®ä½“è¯†åˆ«(NER)
âœ… é—®ç­”ç³»ç»Ÿ
âœ… æ–‡æœ¬ç”Ÿæˆ(GPTç±»æ¨¡å‹)
âœ… ç¿»è¯‘
âœ… æ‘˜è¦ç”Ÿæˆ
âœ… å›¾åƒåˆ†ç±»
âœ… è¯­éŸ³è¯†åˆ«

---

## ğŸ”§ å®‰è£…å’Œé…ç½®

### åŸºç¡€å®‰è£…

```bash
# å®‰è£…transformers
pip install transformers --break-system-packages

# å®‰è£…PyTorchç‰ˆæœ¬
pip install transformers[torch] --break-system-packages

# å®‰è£…TensorFlowç‰ˆæœ¬
pip install transformers[tf] --break-system-packages

# å®Œæ•´å®‰è£…
pip install transformers[all] --break-system-packages
```

### å¸¸ç”¨ä¾èµ–

```bash
# æ•°æ®å¤„ç†
pip install datasets --break-system-packages

# åŠ é€Ÿè®­ç»ƒ
pip install accelerate --break-system-packages

# è¯„ä¼°æŒ‡æ ‡
pip install evaluate --break-system-packages

# æ¨¡å‹ä¼˜åŒ–
pip install optimum --break-system-packages
```

### éªŒè¯å®‰è£…

```python
import transformers
print(f"Transformers version: {transformers.__version__}")

from transformers import pipeline
classifier = pipeline("sentiment-analysis")
print(classifier("I love this library!"))
```

---

## ğŸ’» ä»£ç ç¤ºä¾‹

### 1. ä½¿ç”¨Pipeline (æœ€ç®€å•)

```python
from transformers import pipeline

# æƒ…æ„Ÿåˆ†æ
classifier = pipeline("sentiment-analysis")
result = classifier("I love using Hugging Face!")
print(result)
# [{'label': 'POSITIVE', 'score': 0.9998}]

# æ–‡æœ¬ç”Ÿæˆ
generator = pipeline("text-generation", model="gpt2")
result = generator("Once upon a time", max_length=50)
print(result)

# é—®ç­”
qa_pipeline = pipeline("question-answering")
context = "Hugging Face is a company that provides tools for NLP."
question = "What does Hugging Face provide?"
answer = qa_pipeline(question=question, context=context)
print(answer)

# å‘½åå®ä½“è¯†åˆ«
ner = pipeline("ner", grouped_entities=True)
text = "Hugging Face Inc. is based in New York City"
entities = ner(text)
print(entities)

# ç¿»è¯‘
translator = pipeline("translation_en_to_fr")
result = translator("Hello, how are you?")
print(result)
```

### 2. æ–‡æœ¬åˆ†ç±»(æ‰‹åŠ¨æ–¹å¼)

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# å‡†å¤‡è¾“å…¥
texts = ["I love this product!", "This is terrible."]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

# æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)

print(predictions)
# è·å–æ ‡ç­¾
predicted_labels = predictions.argmax(dim=-1)
print(predicted_labels)
```

### 3. ä½¿ç”¨BERTè¿›è¡Œç‰¹å¾æå–

```python
from transformers import BertTokenizer, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

text = "Hello, my dog is cute"
inputs = tokenizer(text, return_tensors="pt")

# è·å–éšè—çŠ¶æ€
with torch.no_grad():
    outputs = model(**inputs)

# æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
last_hidden_states = outputs.last_hidden_state
print(last_hidden_states.shape)  # [batch_size, seq_length, hidden_size]

# è·å–[CLS] tokençš„è¡¨ç¤º(å¸¸ç”¨äºåˆ†ç±»)
cls_embedding = last_hidden_states[:, 0, :]
print(cls_embedding.shape)
```

### 4. æ–‡æœ¬ç”Ÿæˆ(GPT-2)

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# å‡†å¤‡è¾“å…¥
prompt = "The future of AI is"
inputs = tokenizer(prompt, return_tensors="pt")

# ç”Ÿæˆæ–‡æœ¬
outputs = model.generate(
    **inputs,
    max_length=100,
    num_return_sequences=3,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    do_sample=True
)

# è§£ç ç»“æœ
for i, output in enumerate(outputs):
    text = tokenizer.decode(output, skip_special_tokens=True)
    print(f"Generated {i+1}: {text}\n")
```

### 5. å¾®è°ƒæ¨¡å‹(ä½¿ç”¨Trainer)

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
from datasets import load_dataset

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("imdb")

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)

# æ•°æ®é¢„å¤„ç†
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
    save_strategy="epoch",
    load_best_model_at_end=True,
)

# åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"].select(range(1000)),
    eval_dataset=tokenized_datasets["test"].select(range(200)),
)

# è®­ç»ƒ
trainer.train()

# è¯„ä¼°
results = trainer.evaluate()
print(results)
```

### 6. ä½¿ç”¨ä¸­æ–‡æ¨¡å‹

```python
from transformers import AutoTokenizer, AutoModel

# åŠ è½½ä¸­æ–‡BERT
model_name = "bert-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# å¤„ç†ä¸­æ–‡æ–‡æœ¬
text = "æˆ‘å–œæ¬¢ä½¿ç”¨Hugging Face"
inputs = tokenizer(text, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

print(outputs.last_hidden_state.shape)
```

### 7. æ‰¹é‡å¤„ç†

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")

# æ‰¹é‡å¤„ç†
texts = [
    "I love this!",
    "This is terrible.",
    "It's okay, I guess.",
    "Absolutely amazing!",
    "Worst experience ever."
]

# æ‰¹é‡æ¨ç†
results = classifier(texts, batch_size=8)
for text, result in zip(texts, results):
    print(f"{text}: {result}")
```

### 8. ä¿å­˜å’ŒåŠ è½½æ¨¡å‹

```python
# ä¿å­˜æ¨¡å‹
model.save_pretrained("./my_model")
tokenizer.save_pretrained("./my_model")

# åŠ è½½æ¨¡å‹
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained("./my_model")
tokenizer = AutoTokenizer.from_pretrained("./my_model")
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹

```python
# å°å‹å¿«é€Ÿæ¨¡å‹(æ¨èç”¨äºç”Ÿäº§)
# - DistilBERT (BERTçš„66%å‚æ•°,97%æ€§èƒ½)
# - TinyBERT
# - MobileBERT

# å¹³è¡¡æ¨¡å‹
# - BERT-base
# - RoBERTa-base

# å¤§å‹é«˜æ€§èƒ½æ¨¡å‹
# - BERT-large
# - RoBERTa-large
# - GPT-3

# æ ¹æ®ä»»åŠ¡é€‰æ‹©
task_models = {
    "sentiment": "distilbert-base-uncased-finetuned-sst-2-english",
    "ner": "dslim/bert-base-NER",
    "qa": "distilbert-base-cased-distilled-squad",
    "generation": "gpt2",
    "translation": "Helsinki-NLP/opus-mt-en-zh"
}
```

### 2. ä¼˜åŒ–æ¨ç†é€Ÿåº¦

```python
# 1. ä½¿ç”¨é‡åŒ–
from transformers import AutoModelForSequenceClassification
import torch

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english",
    torchscript=True
)

# åŠ¨æ€é‡åŒ–
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# 2. ä½¿ç”¨ONNX Runtime
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english",
    from_transformers=True
)

# 3. æ‰¹å¤„ç†
texts = ["text1", "text2", "text3"]
classifier(texts, batch_size=8)
```

### 3. å†…å­˜ç®¡ç†

```python
# ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜
model.gradient_checkpointing_enable()

# ä½¿ç”¨8-bitåŠ è½½å¤§æ¨¡å‹
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-560m",
    device_map="auto",
    load_in_8bit=True
)
```

### 4. å¤„ç†é•¿æ–‡æœ¬

```python
# æ–¹æ³•1: æ»‘åŠ¨çª—å£
def process_long_text(text, tokenizer, model, max_length=512, stride=128):
    tokens = tokenizer(text, return_tensors="pt", truncation=False)
    input_ids = tokens["input_ids"][0]

    results = []
    for i in range(0, len(input_ids), max_length - stride):
        chunk = input_ids[i:i + max_length]
        chunk_input = {"input_ids": chunk.unsqueeze(0)}

        with torch.no_grad():
            output = model(**chunk_input)
        results.append(output)

    return results

# æ–¹æ³•2: ä½¿ç”¨Longformeræˆ–BigBird
from transformers import LongformerModel
model = LongformerModel.from_pretrained("allenai/longformer-base-4096")
```

---

## âš ï¸ å¸¸è§é—®é¢˜å’Œæ³¨æ„äº‹é¡¹

### é—®é¢˜1: æ¨¡å‹ä¸‹è½½æ…¢

```python
# æ–¹æ³•1: ä½¿ç”¨é•œåƒ
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# æ–¹æ³•2: æ‰‹åŠ¨ä¸‹è½½ååŠ è½½
model = AutoModel.from_pretrained("./local_model_path")

# æ–¹æ³•3: ä½¿ç”¨huggingface-cli
# huggingface-cli download bert-base-uncased
```

### é—®é¢˜2: æ˜¾å­˜ä¸è¶³

```python
# 1. å‡å°batch size
training_args = TrainingArguments(
    per_device_train_batch_size=8,  # é™ä½
    gradient_accumulation_steps=4   # å¢åŠ 
)

# 2. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# 3. ä½¿ç”¨æ··åˆç²¾åº¦
training_args = TrainingArguments(
    fp16=True  # æˆ– bf16=True
)

# 4. ä½¿ç”¨DeepSpeed
training_args = TrainingArguments(
    deepspeed="ds_config.json"
)
```

### é—®é¢˜3: åˆ†è¯å™¨ç‰¹æ®Šæ ‡è®°

```python
# æŸ¥çœ‹ç‰¹æ®Šæ ‡è®°
print(f"PAD: {tokenizer.pad_token}")
print(f"CLS: {tokenizer.cls_token}")
print(f"SEP: {tokenizer.sep_token}")
print(f"UNK: {tokenizer.unk_token}")

# æ·»åŠ ç‰¹æ®Šæ ‡è®°
tokenizer.add_special_tokens({'additional_special_tokens': ['[CUSTOM]']})
model.resize_token_embeddings(len(tokenizer))
```

---

## ğŸ“– è¿›é˜¶èµ„æº

- [Hugging Face Course](https://huggingface.co/course)
- [Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers)
- [Model Hub](https://huggingface.co/models)
- [Datasetsåº“](https://huggingface.co/docs/datasets)

---

## ğŸ”— ç›¸å…³Skills

- **pytorch-skill**: åº•å±‚æ¡†æ¶
- **spacy-skill**: NLPå¤„ç†
- **fastapi-skill**: æ¨¡å‹éƒ¨ç½²

---

**æœ€åæ›´æ–°**: 2026-01-22
