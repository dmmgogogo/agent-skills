# Pandas Skill

## ğŸ“š å·¥å…·ç®€ä»‹

**Pandas** æ˜¯Pythonä¸­æœ€æµè¡Œçš„æ•°æ®åˆ†æå’Œå¤„ç†åº“ï¼Œæä¾›äº†é«˜æ€§èƒ½ã€æ˜“ç”¨çš„æ•°æ®ç»“æ„å’Œæ•°æ®åˆ†æå·¥å…·ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **DataFrameå’ŒSeries**: å¼ºå¤§çš„æ•°æ®ç»“æ„
- **æ•°æ®æ¸…æ´—**: å¤„ç†ç¼ºå¤±å€¼ã€é‡å¤æ•°æ®
- **æ•°æ®è½¬æ¢**: çµæ´»çš„æ•°æ®æ“ä½œå’Œå˜æ¢
- **æ•°æ®èšåˆ**: åˆ†ç»„ã€é€è§†è¡¨ç­‰é«˜çº§åŠŸèƒ½
- **æ—¶é—´åºåˆ—**: ä¸“ä¸šçš„æ—¶é—´åºåˆ—å¤„ç†èƒ½åŠ›
- **IOå·¥å…·**: æ”¯æŒå¤šç§æ ¼å¼(CSV, Excel, SQL, JSONç­‰)

### GitHubä¿¡æ¯
- **Stars**: 25,000+
- **ä¸‹è½½é‡**: 24äº¿+
- **ä»“åº“**: https://github.com/pandas-dev/pandas
- **å®˜æ–¹æ–‡æ¡£**: https://pandas.pydata.org/

### é€‚ç”¨åœºæ™¯
âœ… ä¸­å°å‹æ•°æ®é›†åˆ†æ (< 1GB)
âœ… æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
âœ… æ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA)
âœ… è´¢åŠ¡åˆ†æå’ŒæŠ¥è¡¨
âœ… æ—¶é—´åºåˆ—åˆ†æ

âŒ è¶…å¤§æ•°æ®é›† (è€ƒè™‘Polarsæˆ–Dask)
âŒ éœ€è¦æè‡´æ€§èƒ½çš„ç”Ÿäº§ç¯å¢ƒ

---

## ğŸ”§ å®‰è£…å’Œé…ç½®

### åŸºç¡€å®‰è£…

```bash
# ä½¿ç”¨pipå®‰è£…
pip install pandas --break-system-packages

# å®‰è£…å®Œæ•´ç‰ˆæœ¬(åŒ…å«æ‰€æœ‰å¯é€‰ä¾èµ–)
pip install pandas[all] --break-system-packages

# å®‰è£…ç‰¹å®šç‰ˆæœ¬
pip install pandas==2.2.0 --break-system-packages
```

### å¸¸ç”¨ä¾èµ–

```bash
# Excelæ”¯æŒ
pip install openpyxl xlrd --break-system-packages

# æ•°æ®åº“æ”¯æŒ
pip install sqlalchemy psycopg2-binary --break-system-packages

# é«˜æ€§èƒ½è®¡ç®—
pip install numpy numexpr bottleneck --break-system-packages

# å¯è§†åŒ–
pip install matplotlib seaborn --break-system-packages
```

### éªŒè¯å®‰è£…

```python
import pandas as pd
print(f"Pandas version: {pd.__version__}")

# æŸ¥çœ‹é…ç½®
pd.show_versions()
```

---

## ğŸ’» ä»£ç ç¤ºä¾‹

### 1. åŸºç¡€æ•°æ®æ“ä½œ

```python
import pandas as pd
import numpy as np

# åˆ›å»ºDataFrame
df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie', 'David'],
    'age': [25, 30, 35, 28],
    'salary': [50000, 60000, 75000, 55000],
    'department': ['HR', 'IT', 'IT', 'Sales']
})

# æŸ¥çœ‹æ•°æ®
print(df.head())
print(df.info())
print(df.describe())

# é€‰æ‹©æ•°æ®
print(df['name'])  # é€‰æ‹©åˆ—
print(df[df['age'] > 28])  # æ¡ä»¶ç­›é€‰
print(df.loc[0:2, ['name', 'age']])  # æ ‡ç­¾ç´¢å¼•
print(df.iloc[0:2, 0:2])  # ä½ç½®ç´¢å¼•
```

### 2. æ•°æ®æ¸…æ´—

```python
# åˆ›å»ºåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®
df = pd.DataFrame({
    'A': [1, 2, np.nan, 4],
    'B': [5, np.nan, np.nan, 8],
    'C': [9, 10, 11, 12]
})

# å¤„ç†ç¼ºå¤±å€¼
df_dropna = df.dropna()  # åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ
df_fillna = df.fillna(0)  # å¡«å……ç¼ºå¤±å€¼
df_fillna_mean = df.fillna(df.mean())  # ç”¨å‡å€¼å¡«å……

# æ£€æµ‹ç¼ºå¤±å€¼
print(df.isnull().sum())

# åˆ é™¤é‡å¤å€¼
df_unique = df.drop_duplicates()

# æ•°æ®ç±»å‹è½¬æ¢
df['A'] = df['A'].astype(int)
```

### 3. æ•°æ®è½¬æ¢å’Œèšåˆ

```python
# åˆ†ç»„èšåˆ
grouped = df.groupby('department').agg({
    'salary': ['mean', 'sum', 'count'],
    'age': 'mean'
})

# é€è§†è¡¨
pivot = df.pivot_table(
    values='salary',
    index='department',
    aggfunc=['mean', 'sum']
)

# æ•°æ®åˆå¹¶
df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})
df2 = pd.DataFrame({'key': ['B', 'C', 'D'], 'value2': [4, 5, 6]})

# å†…è¿æ¥
inner = pd.merge(df1, df2, on='key', how='inner')
# å¤–è¿æ¥
outer = pd.merge(df1, df2, on='key', how='outer')

# æ‹¼æ¥
concatenated = pd.concat([df1, df2], axis=0)
```

### 4. æ—¶é—´åºåˆ—å¤„ç†

```python
# åˆ›å»ºæ—¶é—´åºåˆ—
dates = pd.date_range('2024-01-01', periods=100, freq='D')
ts = pd.Series(np.random.randn(100), index=dates)

# æ—¶é—´ç´¢å¼•
print(ts['2024-01'])  # é€‰æ‹©ç‰¹å®šæœˆä»½
print(ts['2024-01-01':'2024-01-10'])  # æ—¶é—´èŒƒå›´

# é‡é‡‡æ ·
monthly = ts.resample('M').mean()  # æŒ‰æœˆèšåˆ
weekly = ts.resample('W').sum()  # æŒ‰å‘¨æ±‚å’Œ

# æ—¶é—´çª—å£
rolling_mean = ts.rolling(window=7).mean()  # 7å¤©ç§»åŠ¨å¹³å‡
```

### 5. è¯»å†™æ–‡ä»¶

```python
# CSV
df.to_csv('data.csv', index=False)
df = pd.read_csv('data.csv')

# Excel
df.to_excel('data.xlsx', sheet_name='Sheet1', index=False)
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')

# JSON
df.to_json('data.json', orient='records')
df = pd.read_json('data.json')

# SQL
from sqlalchemy import create_engine
engine = create_engine('sqlite:///database.db')
df.to_sql('table_name', engine, if_exists='replace')
df = pd.read_sql('SELECT * FROM table_name', engine)

# Parquet (é«˜æ•ˆçš„åˆ—å¼å­˜å‚¨)
df.to_parquet('data.parquet')
df = pd.read_parquet('data.parquet')
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–

```python
# ä½¿ç”¨å‘é‡åŒ–æ“ä½œ,é¿å…å¾ªç¯
# âŒ ä¸å¥½çš„åšæ³•
for i in range(len(df)):
    df.loc[i, 'new_col'] = df.loc[i, 'A'] * 2

# âœ… å¥½çš„åšæ³•
df['new_col'] = df['A'] * 2

# ä½¿ç”¨ç±»åˆ«ç±»å‹èŠ‚çœå†…å­˜
df['category_col'] = df['category_col'].astype('category')

# åˆ†å—è¯»å–å¤§æ–‡ä»¶
chunks = pd.read_csv('large_file.csv', chunksize=10000)
for chunk in chunks:
    process(chunk)

# ä½¿ç”¨queryæ–¹æ³•æé«˜å¯è¯»æ€§å’Œæ€§èƒ½
result = df.query('age > 25 and salary < 60000')
```

### 2. å†…å­˜ç®¡ç†

```python
# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
print(df.memory_usage(deep=True))

# ä¼˜åŒ–æ•°æ®ç±»å‹
def optimize_dtypes(df):
    for col in df.select_dtypes(include=['int']).columns:
        df[col] = pd.to_numeric(df[col], downcast='integer')
    for col in df.select_dtypes(include=['float']).columns:
        df[col] = pd.to_numeric(df[col], downcast='float')
    return df

df = optimize_dtypes(df)
```

### 3. é“¾å¼æ“ä½œ

```python
# ä½¿ç”¨æ–¹æ³•é“¾æé«˜ä»£ç å¯è¯»æ€§
result = (df
    .query('age > 25')
    .groupby('department')
    .agg({'salary': 'mean'})
    .sort_values('salary', ascending=False)
    .reset_index()
)
```

### 4. æ•°æ®éªŒè¯

```python
# ä½¿ç”¨assertè¿›è¡Œæ•°æ®éªŒè¯
assert df['age'].min() >= 0, "å¹´é¾„ä¸èƒ½ä¸ºè´Ÿ"
assert df.duplicated().sum() == 0, "å­˜åœ¨é‡å¤æ•°æ®"
assert df.isnull().sum().sum() == 0, "å­˜åœ¨ç¼ºå¤±å€¼"
```

---

## âš ï¸ å¸¸è§é—®é¢˜å’Œæ³¨æ„äº‹é¡¹

### é—®é¢˜1: SettingWithCopyWarning

```python
# âŒ ä¼šè§¦å‘è­¦å‘Š
subset = df[df['age'] > 25]
subset['new_col'] = 1  # è­¦å‘Š!

# âœ… æ­£ç¡®åšæ³•
subset = df[df['age'] > 25].copy()
subset['new_col'] = 1
```

### é—®é¢˜2: é“¾å¼ç´¢å¼•

```python
# âŒ é“¾å¼ç´¢å¼•(å¯èƒ½ä¸å·¥ä½œ)
df[df['A'] > 0]['B'] = 1

# âœ… ä½¿ç”¨loc
df.loc[df['A'] > 0, 'B'] = 1
```

### é—®é¢˜3: å†…å­˜æº¢å‡º

```python
# å¯¹äºå¤§æ–‡ä»¶,ä½¿ç”¨åˆ†å—è¯»å–
chunks = []
for chunk in pd.read_csv('large.csv', chunksize=10000):
    processed = process_chunk(chunk)
    chunks.append(processed)
result = pd.concat(chunks)
```

### é—®é¢˜4: æ—¥æœŸè§£ææ…¢

```python
# âŒ è‡ªåŠ¨æ¨æ–­æ—¥æœŸ(æ…¢)
df = pd.read_csv('data.csv', parse_dates=True)

# âœ… æ˜ç¡®æŒ‡å®šæ—¥æœŸåˆ—å’Œæ ¼å¼
df = pd.read_csv('data.csv',
                 parse_dates=['date_col'],
                 date_format='%Y-%m-%d')
```

### é—®é¢˜5: æ€§èƒ½å¯¹æ¯”

å½“æ•°æ®é‡ > 1GB æˆ–éœ€è¦æè‡´æ€§èƒ½æ—¶:
- è€ƒè™‘ **Polars** (30xæ€§èƒ½æå‡)
- è€ƒè™‘ **Dask** (åˆ†å¸ƒå¼å¤„ç†)
- è€ƒè™‘ **DuckDB** (SQLåˆ†æ)

---

## ğŸ“– è¿›é˜¶èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Pandaså®˜æ–¹æ–‡æ¡£](https://pandas.pydata.org/docs/)
- [10åˆ†é’Ÿå…¥é—¨Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)
- [Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html)

### æ¨èä¹¦ç±
- "Python for Data Analysis" by Wes McKinney (Pandasä½œè€…)
- "Pandas Cookbook" by Matt Harrison

### åœ¨çº¿æ•™ç¨‹
- [Kaggle Learn - Pandas](https://www.kaggle.com/learn/pandas)
- [DataCamp - Pandas Courses](https://www.datacamp.com/courses/pandas-foundations)

---

## ğŸ”— ç›¸å…³Skills

- **numpy-skill**: Pandasçš„åº•å±‚ä¾èµ–
- **polars-skill**: é«˜æ€§èƒ½æ›¿ä»£æ–¹æ¡ˆ
- **matplotlib-skill**: æ•°æ®å¯è§†åŒ–
- **jupyter-skill**: äº¤äº’å¼å¼€å‘ç¯å¢ƒ

---

**æœ€åæ›´æ–°**: 2026-01-22
**ç‰ˆæœ¬**: 2.2.x
